{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% not necessary if using python -m\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from fractions import Fraction\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from util import step_plot, ds_names, ds_names_short, colors_full, linestyles, colors_group\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p_CE = Path('../data/clique_expansion')\n",
    "p_LCC = Path('../data/largest_connected_component')\n",
    "p_LCC.mkdir(exist_ok=True)\n",
    "for ds_name in ds_names:\n",
    "    p_CE_ds = p_CE / ds_name\n",
    "    with (p_CE_ds / 'CE_U.pkl').open('rb') as f:\n",
    "        CE_U = pickle.load(f)\n",
    "    LCC = max(nx.connected_components(CE_U), key=len)\n",
    "    with (p_LCC / '{}.pkl'.format(ds_name)).open('wb') as f:\n",
    "        pickle.dump(LCC, f)\n",
    "    print('Dataset {},'.format(ds_name), 'total number of nodes = {},'.format(CE_U.number_of_nodes()),\n",
    "          'number of nodes in the LCC = {}'.format(len(LCC)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "p_CE = Path('../data/clique_expansion')\n",
    "p_EC = Path('../data/eigenvector_centrality')\n",
    "p_incident = Path('../data/incident')\n",
    "p_EC.mkdir(exist_ok=True)\n",
    "centrality_types = ['linear', 'log-exp', 'max']\n",
    "max_iter = 1000\n",
    "tol = 1e-6\n",
    "for ds_name in ds_names[4:]:\n",
    "    print(ds_name)\n",
    "    p_CE_ds = p_CE / ds_name\n",
    "    p_EC_ds = p_EC / ds_name\n",
    "    p_EC_ds.mkdir(exist_ok=True)\n",
    "    with (p_CE_ds / 'CE_U.pkl').open('rb') as f:\n",
    "        CE_U = pickle.load(f)\n",
    "    with (p_CE_ds / 'CE_W.pkl').open('rb') as f:\n",
    "        CE_W = pickle.load(f)\n",
    "\n",
    "    EC_CE_U = nx.eigenvector_centrality(CE_U)\n",
    "    with (p_EC_ds / 'EC_CE_U.pkl').open('wb') as f:\n",
    "        pickle.dump(EC_CE_U, f)\n",
    "    print('Eigen-centrality of unweighted clique expansion done!')\n",
    "\n",
    "    CE_W_conv = nx.Graph()\n",
    "    for v1, v2 in CE_W.edges():\n",
    "        if CE_W_conv.has_edge(v1, v2):\n",
    "            CE_W_conv[v1][v2]['weight'] += 1\n",
    "        else:\n",
    "            CE_W_conv.add_edge(v1, v2, weight=1)\n",
    "    EC_CE_W = nx.eigenvector_centrality(CE_W_conv, weight='weight', max_iter=max_iter)\n",
    "    with (p_EC_ds / 'EC_CE_W.pkl').open('wb') as f:\n",
    "        pickle.dump(EC_CE_W, f)\n",
    "    print('Eigen-centrality of weighted clique expansion done!')\n",
    "\n",
    "    p_incident_ds = p_incident / ds_name\n",
    "    with (p_incident_ds / 'i2edges.pkl').open('rb') as f:\n",
    "        i2edges = pickle.load(f)\n",
    "    with (p_incident_ds / 'v2edges.pkl').open('rb') as f:\n",
    "        v2edges = pickle.load(f)\n",
    "\n",
    "    n, m = len(v2edges), len(i2edges)\n",
    "    node_list = sorted(v2edges.keys())\n",
    "    row_list = []\n",
    "    col_list = []\n",
    "    for i_v, v in enumerate(node_list):\n",
    "        for e in v2edges[v]:\n",
    "            row_list.append(i_v)\n",
    "            col_list.append(e)\n",
    "    data = [True for _ in row_list]\n",
    "    B = csr_matrix((data, (row_list, col_list)), shape=(n, m), dtype=bool)\n",
    "\n",
    "    for centrality_type in centrality_types:\n",
    "        if centrality_type == 'linear':\n",
    "            f = g = phi = psi = lambda xx: xx\n",
    "        elif centrality_type == 'log-exp':\n",
    "            f = lambda xx: xx\n",
    "            g = lambda xx: np.sqrt(xx)\n",
    "            phi = lambda xx: np.log(xx)\n",
    "            psi = lambda xx: np.exp(xx)\n",
    "        else:  # 'max'\n",
    "            f = g = lambda xx: xx\n",
    "            phi = lambda xx: xx ** 10\n",
    "            psi = lambda xx: xx ** 0.1\n",
    "        f = np.vectorize(f)\n",
    "        g = np.vectorize(g)\n",
    "        phi = np.vectorize(phi)\n",
    "        psi = np.vectorize(psi)\n",
    "\n",
    "        x0 = np.ones(n) / n\n",
    "        y0 = np.ones(m) / m\n",
    "        diff = x = y = None\n",
    "        for _ in range(max_iter):\n",
    "            u = np.sqrt(x0 * g(B.dot(f(y0))))\n",
    "            v = np.sqrt(y0 * psi(B.T.dot(phi(x0))))\n",
    "            x = u / np.linalg.norm(u)\n",
    "            y = v / np.linalg.norm(v)\n",
    "            diff = np.linalg.norm(x - x0) / np.linalg.norm(x) + np.linalg.norm(y - y0) / np.linalg.norm(y)\n",
    "            if diff < tol:\n",
    "                break\n",
    "            x0 = np.copy(x)\n",
    "            y0 = np.copy(y)\n",
    "        print(ds_name, centrality_type, diff)\n",
    "        centrality_dict_V = dict(zip(node_list, x))\n",
    "        centrality_dict_E = dict(zip(range(m), y))\n",
    "\n",
    "        with (p_EC_ds / 'hyperEC_{}_V.pkl'.format(centrality_type)).open('wb') as f:\n",
    "            pickle.dump(centrality_dict_V, f)\n",
    "        with (p_EC_ds / 'hyperEC_{}_E.pkl'.format(centrality_type)).open('wb') as f:\n",
    "            pickle.dump(centrality_dict_E, f)\n",
    "        print('Hyper-eigen-centrality ({}) done!'.format(centrality_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data_cpp = Path('../data_cpp')\n",
    "p_data_cpp.mkdir(exist_ok=True)\n",
    "p_incident = Path('../data/incident')\n",
    "p_LCC = Path('../data/largest_connected_component')\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    print(ds_name)\n",
    "    p_incident_ds = p_incident / ds_name\n",
    "    with (p_incident_ds / 'i2edges.pkl').open('rb') as f:\n",
    "        i2edges = pickle.load(f)\n",
    "    with (p_incident_ds / 'v2edges.pkl').open('rb') as f:\n",
    "        v2edges = pickle.load(f)\n",
    "\n",
    "    with open(p_data_cpp / '{}_edges.txt'.format(ds_name), 'w') as f:\n",
    "        for e in i2edges.values():\n",
    "            f.write('\\t'.join([str(v) for v in e]) + '\\n')\n",
    "\n",
    "    with open(p_data_cpp / '{}_degrees.txt'.format(ds_name), 'w') as f:\n",
    "        for v, E_v in v2edges.items():\n",
    "            f.write('{}\\t{}\\n'.format(v, len(E_v)))\n",
    "\n",
    "    with open(p_data_cpp / '{}_incident.txt'.format(ds_name), 'w') as f:\n",
    "        for v, E_v in v2edges.items():\n",
    "            f.write(str(v) + '\\t' + ','.join([str(e) for e in E_v]) + '\\n')\n",
    "\n",
    "    with (p_LCC / '{}.pkl'.format(ds_name)).open('rb') as f:\n",
    "        LCC = pickle.load(f)\n",
    "\n",
    "    with open(p_data_cpp / '{}_cc.txt'.format(ds_name), 'w') as f:\n",
    "        for v in LCC:\n",
    "            f.write('{}\\n'.format(v))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
